{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2539039,"sourceType":"datasetVersion","datasetId":1539344}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-19T09:19:17.880970Z","iopub.execute_input":"2024-04-19T09:19:17.881531Z","iopub.status.idle":"2024-04-19T09:19:17.926269Z","shell.execute_reply.started":"2024-04-19T09:19:17.881478Z","shell.execute_reply":"2024-04-19T09:19:17.924644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Task : Perform hyper-parameter tuning for Regression models. \n\n       2a. Linear Regression: Grid Search \n       \n       2b. Random Forest: Random Search  \n       \n       2c. Xgboost: Random Search","metadata":{}},{"cell_type":"markdown","source":"The problem statement provides pickup details of customers using cab aggregator. It provides low level details of every pickup location, time and other further details w.r.t ride.\n\nTask at hand is to determine the demand. Demand can be termed as the number of pickups made by taxi aggregator at a specific time. This demand can be converted into percentage by dividing with maximum number of pickups that can happen at any hour. This is precisely what we have done.\n\nThe location data is converted to location clusters using clustering algorithm. The datetime of pickup is transformed and we extract time as a useful feature from pickup datetime. With location cluster & pickup hour, we create our models. \n\nAfter validating multiple regression models such as LR, RF, XGBoost, we decided on using XGBoost after loosing all the not so important features.\n\nAn inference function is also written at the end that will be useful when we try to deploy this as a model for a POC given we save the XGboost and Kmeans and use those in inferencing.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/taxidemandfarepredictiondataset/yellow_tripdata_2015-02.csv\")\ndf = df.sample(frac=0.005)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:19:25.018076Z","iopub.execute_input":"2024-04-19T09:19:25.018543Z","iopub.status.idle":"2024-04-19T09:20:34.850173Z","shell.execute_reply.started":"2024-04-19T09:19:25.018503Z","shell.execute_reply":"2024-04-19T09:20:34.848790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:00.938419Z","iopub.execute_input":"2024-04-19T09:21:00.939346Z","iopub.status.idle":"2024-04-19T09:21:00.985631Z","shell.execute_reply.started":"2024-04-19T09:21:00.939305Z","shell.execute_reply":"2024-04-19T09:21:00.984663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape, df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:05.401922Z","iopub.execute_input":"2024-04-19T09:21:05.402362Z","iopub.status.idle":"2024-04-19T09:21:05.410565Z","shell.execute_reply.started":"2024-04-19T09:21:05.402326Z","shell.execute_reply":"2024-04-19T09:21:05.408947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:09.037118Z","iopub.execute_input":"2024-04-19T09:21:09.037593Z","iopub.status.idle":"2024-04-19T09:21:09.047608Z","shell.execute_reply.started":"2024-04-19T09:21:09.037555Z","shell.execute_reply":"2024-04-19T09:21:09.046630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['RateCodeID'].value_counts()\ndf['payment_type'].value_counts()\ndf['store_and_fwd_flag'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:12.897995Z","iopub.execute_input":"2024-04-19T09:21:12.899065Z","iopub.status.idle":"2024-04-19T09:21:12.925398Z","shell.execute_reply.started":"2024-04-19T09:21:12.898984Z","shell.execute_reply":"2024-04-19T09:21:12.924105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']] = df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].apply(pd.to_datetime)\ndf[['VendorID','RateCodeID','payment_type', 'store_and_fwd_flag' ]] = df[['VendorID','RateCodeID','payment_type', 'store_and_fwd_flag' ]].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:18.361081Z","iopub.execute_input":"2024-04-19T09:21:18.361517Z","iopub.status.idle":"2024-04-19T09:21:18.689651Z","shell.execute_reply.started":"2024-04-19T09:21:18.361482Z","shell.execute_reply":"2024-04-19T09:21:18.688199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all').T\n\n# std deviation on trip distance along with max is really high. lets explore this further\n# last in tpep_dropoff_datetime looks impossible realistically. lets explore this further\n# passenger_count max is an outlier\n# -ve values in min of amount columns shows the cab service works on credit as well. Let's see how many such entries are there.\n# Overall these above ideas will help us to remove outliers and overall rows from dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:21.806245Z","iopub.execute_input":"2024-04-19T09:21:21.806663Z","iopub.status.idle":"2024-04-19T09:21:22.014935Z","shell.execute_reply.started":"2024-04-19T09:21:21.806629Z","shell.execute_reply":"2024-04-19T09:21:22.013336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates(keep='last')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:26.471473Z","iopub.execute_input":"2024-04-19T09:21:26.471937Z","iopub.status.idle":"2024-04-19T09:21:26.566546Z","shell.execute_reply.started":"2024-04-19T09:21:26.471900Z","shell.execute_reply":"2024-04-19T09:21:26.565201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:29.741878Z","iopub.execute_input":"2024-04-19T09:21:29.742344Z","iopub.status.idle":"2024-04-19T09:21:29.790286Z","shell.execute_reply.started":"2024-04-19T09:21:29.742306Z","shell.execute_reply":"2024-04-19T09:21:29.789292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove pickup and drops outside newyork city limits\n\ndf.drop(df.index[\n        ~((df['pickup_latitude'].between(40.496115395170364, 40.91553277700258)) &\n          (df['pickup_longitude'].between(-74.25559136315209, -73.7000090639354))) \n], inplace=True)\n\ndf.drop(df.index[\n        ~((df['dropoff_latitude'].between(40.496115395170364, 40.91553277700258)) &\n          (df['dropoff_longitude'].between(-74.25559136315209, -73.7000090639354))) \n], inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:21:34.734246Z","iopub.execute_input":"2024-04-19T09:21:34.735404Z","iopub.status.idle":"2024-04-19T09:21:34.774258Z","shell.execute_reply.started":"2024-04-19T09:21:34.735359Z","shell.execute_reply":"2024-04-19T09:21:34.772807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:01.570411Z","iopub.execute_input":"2024-04-19T09:22:01.570935Z","iopub.status.idle":"2024-04-19T09:22:01.587620Z","shell.execute_reply.started":"2024-04-19T09:22:01.570894Z","shell.execute_reply":"2024-04-19T09:22:01.586611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\ndef bplot(df, a):\n    sns.set_style('whitegrid')\n    sns.boxplot(y=a, data=df, width=0.4)\n    plt.show()\n\nbplot(df, 'trip_distance')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:05.249161Z","iopub.execute_input":"2024-04-19T09:22:05.249653Z","iopub.status.idle":"2024-04-19T09:22:06.324008Z","shell.execute_reply.started":"2024-04-19T09:22:05.249614Z","shell.execute_reply":"2024-04-19T09:22:06.322546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def computer_remove_outliers(df, x, remove_outlier=1):\n    q1 = df[x].quantile(0.25)\n    q3 = df[x].quantile(0.75)\n    iqr = q3-q1\n    ll = q1 - 1.5*(iqr)\n    ul = q3 + 1.5*(iqr)\n    print('lower limit of dist is {} and upper limit of dist is {}'.format(ll, ul))\n\n    # remove all negative distances and distances greater than ul\n    if remove_outlier==1:\n        df = df[(df[x]>ll) & (df[x]<ul)]\n        df.shape\n        return df\n\ndf = computer_remove_outliers(df, 'trip_distance')\ndf = df[df['trip_distance'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:10.928263Z","iopub.execute_input":"2024-04-19T09:22:10.928673Z","iopub.status.idle":"2024-04-19T09:22:10.966991Z","shell.execute_reply.started":"2024-04-19T09:22:10.928638Z","shell.execute_reply":"2024-04-19T09:22:10.965588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify outliers in date columns - trips more than n hours could be outliers.\ndf['time_diff_hours'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).astype('timedelta64[h]')\ndf['time_diff_hours'] = df['time_diff_hours'].astype(int)\nbplot(df, 'time_diff_hours')\n\n(df['time_diff_hours'].value_counts().sort_values(ascending=False))\n# based on above statement output, time diff greater than 1 can be neglected\ndf = df[df[\"time_diff_hours\"] < 2]\ndf.shape\n\n# remove cancelled rides\ndf['time_diff_min'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).astype('timedelta64[m]')\ndf[df['time_diff_min']==0]\ndf = df[df[\"time_diff_min\"] > 0]  \n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:15.490206Z","iopub.execute_input":"2024-04-19T09:22:15.490628Z","iopub.status.idle":"2024-04-19T09:22:15.703788Z","shell.execute_reply.started":"2024-04-19T09:22:15.490593Z","shell.execute_reply":"2024-04-19T09:22:15.702201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove uncommon passenger counts\ndf[\"passenger_count\"].value_counts(normalize=True).sort_values(ascending=False) * 100\ndf = df[(df[\"passenger_count\"] <=2) & (df[\"passenger_count\"] >0)]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:21.642195Z","iopub.execute_input":"2024-04-19T09:22:21.642630Z","iopub.status.idle":"2024-04-19T09:22:21.668935Z","shell.execute_reply.started":"2024-04-19T09:22:21.642584Z","shell.execute_reply":"2024-04-19T09:22:21.667427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[(df['total_amount'] > 0) & (df['total_amount'] < 500)]\ndf.shape\n\ndf = df[df['extra'] >= 0]\ndf.shape\n\nbplot(df, 'total_amount')\n\nsns.set_style('whitegrid')\nsns.histplot(kde=True, data=df['total_amount'], stat='density')\nsns.ecdfplot(df['total_amount'])\nplt.show()\n\ncomputer_remove_outliers(df, 'total_amount', 0)\n\n# based on above analysis, we can easily remove total amount > 200 even though the max from box plot came out as $25\ndf = df[df['total_amount'] < 200]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:25.450563Z","iopub.execute_input":"2024-04-19T09:22:25.451008Z","iopub.status.idle":"2024-04-19T09:22:27.595731Z","shell.execute_reply.started":"2024-04-19T09:22:25.450972Z","shell.execute_reply":"2024-04-19T09:22:27.594100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_location = df[['pickup_latitude', 'pickup_longitude']]","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:22:34.407682Z","iopub.execute_input":"2024-04-19T09:22:34.409331Z","iopub.status.idle":"2024-04-19T09:22:34.416731Z","shell.execute_reply.started":"2024-04-19T09:22:34.409266Z","shell.execute_reply":"2024-04-19T09:22:34.415356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # Elbox plot to find k in k means\n# from sklearn.cluster import KMeans\n\nsse = []\nK = range(1,20, 2)\nfor k in K:\n     kmeanModel = KMeans(n_clusters=k)\n     kmeanModel.fit(df_location)\n     sse.append(kmeanModel.inertia_)\n\nplt.figure(figsize=(15,6))\nplt.plot(K, sse, 'bx-')\nplt.xlabel('Number of Clusters (k)', fontsize = 15)\nplt.ylabel('Sum of Squared Error', fontsize = 15)\nplt.title('The Elbow Method showing the optimal k', fontsize = 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:20.189313Z","iopub.execute_input":"2024-04-19T09:23:20.189838Z","iopub.status.idle":"2024-04-19T09:23:20.322667Z","shell.execute_reply.started":"2024-04-19T09:23:20.189796Z","shell.execute_reply":"2024-04-19T09:23:20.320915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_location.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:40:48.169308Z","iopub.execute_input":"2023-04-05T04:40:48.169727Z","iopub.status.idle":"2023-04-05T04:40:48.185891Z","shell.execute_reply.started":"2023-04-05T04:40:48.169691Z","shell.execute_reply":"2023-04-05T04:40:48.184394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # from elbow plot, we see that SSE reduces slowly after k value __ . Still, lets try silhoutte score for many k values.\n# from sklearn.metrics import silhouette_score\n\n# print(\"Clusters\\tSilhoutte Score\\n\")\n# for n_cluster in range(5, 15):\n#     kmeans = KMeans(n_clusters=n_cluster).fit(df_location)\n#     label = kmeans.labels_\n#     sil_coeff = silhouette_score(df_location, label, metric='euclidean')\n#     print(\"k = {} \\t--> \\t{}\".format(n_cluster, sil_coeff))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:40:48.187972Z","iopub.execute_input":"2023-04-05T04:40:48.188509Z","iopub.status.idle":"2023-04-05T04:40:48.19654Z","shell.execute_reply.started":"2023-04-05T04:40:48.188427Z","shell.execute_reply":"2023-04-05T04:40:48.195176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters = 10) \ndf_location['cluster_label'] = kmeans.fit_predict(df_location)  # df_location has lat and long in a DF.\ndf_location['cluster_label'].value_counts().sort_values(ascending=False)\n\nsns.set_style('whitegrid')\nsns.scatterplot(x='pickup_latitude', y = 'pickup_longitude', data=(df_location),hue='cluster_label', palette='pastel')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:34.970694Z","iopub.execute_input":"2024-04-19T09:23:34.971259Z","iopub.status.idle":"2024-04-19T09:23:42.504562Z","shell.execute_reply.started":"2024-04-19T09:23:34.971206Z","shell.execute_reply":"2024-04-19T09:23:42.503458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_loc_clusters = pd.merge(df_location, df, on=['pickup_latitude', 'pickup_longitude'])","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:47.760469Z","iopub.execute_input":"2024-04-19T09:23:47.760940Z","iopub.status.idle":"2024-04-19T09:23:47.868287Z","shell.execute_reply.started":"2024-04-19T09:23:47.760893Z","shell.execute_reply":"2024-04-19T09:23:47.866731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_loc_clusters['pickup_hour'] = df_with_loc_clusters[\"tpep_pickup_datetime\"].dt.hour\ndf_with_loc_clusters['pickup_dayofweek'] = df_with_loc_clusters[\"tpep_pickup_datetime\"].dt.dayofweek","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:50.629566Z","iopub.execute_input":"2024-04-19T09:23:50.630118Z","iopub.status.idle":"2024-04-19T09:23:50.649477Z","shell.execute_reply.started":"2024-04-19T09:23:50.630074Z","shell.execute_reply":"2024-04-19T09:23:50.648275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n\ndf_with_loc_clusters.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:53.704563Z","iopub.execute_input":"2024-04-19T09:23:53.704997Z","iopub.status.idle":"2024-04-19T09:23:53.742989Z","shell.execute_reply.started":"2024-04-19T09:23:53.704960Z","shell.execute_reply":"2024-04-19T09:23:53.741620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_model = df_with_loc_clusters.groupby(['pickup_dayofweek','pickup_hour', 'cluster_label']).size().reset_index()\ndf_model = df_model.rename(columns={0:'demand'})\ndf_model['demand'] = df_model['demand']/df_model['demand'].max()\ndf_model['demand'] = round((df_model['demand'] * 100), 2)\ndf_model.head()\ndf_model.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:23:57.595671Z","iopub.execute_input":"2024-04-19T09:23:57.596156Z","iopub.status.idle":"2024-04-19T09:23:57.643834Z","shell.execute_reply.started":"2024-04-19T09:23:57.596100Z","shell.execute_reply":"2024-04-19T09:23:57.641970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_model.drop([\"demand\"],axis=1)\ny = df_model[['demand']]\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=100)\n\nfrom sklearn.preprocessing import OneHotEncoder\ncat_columns = ['pickup_dayofweek', 'pickup_hour', 'cluster_label']\nenc = OneHotEncoder()\nenc.fit(X_train[cat_columns])\nX_train_encoded = enc.transform(X_train[cat_columns]).toarray()\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=enc.get_feature_names(cat_columns))\nX_train_model = pd.concat([X_train.reset_index(), X_train_encoded_df], axis=1).drop(['index'], axis=1)\n\nX_test_encoded = enc.transform(X_test[cat_columns]).toarray()\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=enc.get_feature_names(cat_columns))\nX_test_model = pd.concat([X_test.reset_index(), X_test_encoded_df], axis=1).drop(['index'], axis=1)\n\nX_train_model = X_train_model.drop(cat_columns, axis=1)\nX_test_model = X_test_model.drop(cat_columns, axis=1)\n\nX_train_model[X_train_model.columns] = X_train_model[X_train_model.columns].astype(int)\nX_test_model[X_test_model.columns] = X_test_model[X_test_model.columns].astype(int)\n\ny_train_model = y_train.reset_index().drop(['index'], axis=1)\ny_test_model = y_test.reset_index().drop(['index'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:24:00.874149Z","iopub.execute_input":"2024-04-19T09:24:00.874644Z","iopub.status.idle":"2024-04-19T09:24:00.936292Z","shell.execute_reply.started":"2024-04-19T09:24:00.874604Z","shell.execute_reply":"2024-04-19T09:24:00.934225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\ny_train_model = y_train_model['demand']\ny_test_model = y_test_model['demand']\n\nmodels = []\nmodels.append(('LinearRegression',LinearRegression()))\nmodels.append(('RandomForest' ,RandomForestRegressor()))  # RF is bound to overfit and we will have to tune it later.\nmodels.append(('xgboost' ,XGBRegressor()))  # RF is bound to overfit and we will have to tune it later.\n\n\nfor name , model in models :\n    model.fit(X_train_model , y_train_model)\n    \n    prediction = model.predict(X_test_model)\n    \n    mse = mean_squared_error(prediction, y_test_model)\n    r2 = model.score(X_test_model,y_test_model)\n    print('{}: MSE is {} and R2 is {}'.format(name, round(mse,2), round(r2,2)))\n    print('\\n')\n    \n    \n# Base model selected is XGboost. RandomForest is also close. So, lets expand further in these 2 algorithms.","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:24:07.799714Z","iopub.execute_input":"2024-04-19T09:24:07.800746Z","iopub.status.idle":"2024-04-19T09:24:09.220493Z","shell.execute_reply.started":"2024-04-19T09:24:07.800701Z","shell.execute_reply":"2024-04-19T09:24:09.218968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hpyerparameter tuning for XGBoost algorithm\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nxgb_model = XGBRegressor(eval_metric=mean_squared_error)\nparam_dist = {\"max_depth\": [2,5,7],  # 5\n              \"n_estimators\": [50, 70, 100, 300, 500, 1000],  # 100\n              \"learning_rate\" : [0.01,0.05, 0.1,0.3, 0.5]}  # 0.1\ngrid_search = GridSearchCV(xgb_model, param_grid=param_dist, cv = 3, \n                                   verbose=1, n_jobs=-1)\ngrid_search.fit(X_train_model, y_train_model.ravel())\nprint(grid_search.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:24:29.559907Z","iopub.execute_input":"2024-04-19T09:24:29.561560Z","iopub.status.idle":"2024-04-19T09:26:52.714830Z","shell.execute_reply.started":"2024-04-19T09:24:29.561494Z","shell.execute_reply":"2024-04-19T09:26:52.713081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\n\n# Define models\nmodels = []\nmodels.append(('LinearRegression', LinearRegression()))\nmodels.append(('SVR', SVR()))\nmodels.append(('SGD', SGDRegressor()))\nmodels.append(('AdaBoost', AdaBoostRegressor()))\nmodels.append(('RandomForest', RandomForestRegressor()))  # RF is bound to overfit and we will have to tune it later.\nmodels.append(('XGBoost', XGBRegressor()))\nmodels.append(('GradientBoosting', GradientBoostingRegressor()))\n\n# Training and evaluation loop\nfor name, model in models:\n    print(\"Training {}...\".format(name))\n    model.fit(X_train_model, y_train_model)\n    \n    # Evaluate on training set\n    prediction_train = model.predict(X_train_model)\n    mse_train = mean_squared_error(prediction_train, y_train_model)\n    r2_train = model.score(X_train_model, y_train_model)\n    print('{}: Training MSE is {} and R2 is {}'.format(name, round(mse_train, 2), round(r2_train, 2)))\n\n    # Evaluate on test set\n    prediction_test = model.predict(X_test_model)\n    mse_test = mean_squared_error(prediction_test, y_test_model)\n    r2_test = model.score(X_test_model, y_test_model)\n    print('{}: Test MSE is {} and R2 is {}'.format(name, round(mse_test, 2), round(r2_test, 2)))\n\n    print('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:35:09.439767Z","iopub.execute_input":"2024-04-19T09:35:09.440343Z","iopub.status.idle":"2024-04-19T09:35:11.491867Z","shell.execute_reply.started":"2024-04-19T09:35:09.440294Z","shell.execute_reply":"2024-04-19T09:35:11.490496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training XGboost using hyper parameters selected using grid search CV and using important features only.\n\nparams = {\"max_depth\": 5, \"learning_rate\": 0.1, \"n_estimators\": 1000}\nmodel = XGBRegressor(**params)\neval_set = [(X_test_model_impfeatures, y_test_model)]\nmodel.fit(X_train_model_impfeatures , y_train_model, eval_set=eval_set, early_stopping_rounds=10)  # , verbose = True)\n\nprediction_train = model.predict(X_train_model_impfeatures)\nmse = mean_squared_error(prediction_train, y_train_model)\nr2 = model.score(X_train_model_impfeatures,y_train_model)\nprint('{}: MSE is {} and R2 is {}'.format('XGBoost on Train', round(mse,2), round(r2,2)))\nprint('\\n')\n\nprediction = model.predict(X_test_model_impfeatures)\nmse = mean_squared_error(prediction, y_test_model)\nr2 = model.score(X_test_model_impfeatures,y_test_model)\nprint('{}: MSE is {} and R2 is {}'.format('XGBoost on Test', round(mse,2), round(r2,2)))\nprint('\\n')\n\n# This gives a better result as unimportant features are removed and we do not see overfitting even though we see some drop in metrics","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:27:11.842876Z","iopub.execute_input":"2024-04-19T09:27:11.844326Z","iopub.status.idle":"2024-04-19T09:27:13.690697Z","shell.execute_reply.started":"2024-04-19T09:27:11.844261Z","shell.execute_reply":"2024-04-19T09:27:13.689386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hpyerparameter tuning for Random Forest algorithm\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nrf_model = RandomForestRegressor()\nparam_dist = { \n    'n_estimators': [200, 500, 1000],  # trees in forest\n    'min_samples_split': [2, 5, 7],\n    'min_samples_leaf': [1, 2],\n    'max_features': ['sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8]  \n}\ngrid_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, cv = 3, \n                                   verbose=1, n_jobs=-1)\ngrid_search.fit(X_train_model, y_train_model)\nprint(grid_search.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:46:12.597422Z","iopub.execute_input":"2023-04-05T04:46:12.600533Z","iopub.status.idle":"2023-04-05T04:46:25.536812Z","shell.execute_reply.started":"2023-04-05T04:46:12.600475Z","shell.execute_reply":"2023-04-05T04:46:25.535512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training random forest using hyper parameters selected using randomized search CV.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nparams = {\"max_depth\": 8, \"min_samples_split\": 7, \"n_estimators\": 1000}\nmodel_rf = RandomForestRegressor(**params)\n# eval_set = [(X_test_model, y_test_model)]\nmodel_rf.fit(X_train_model , y_train_model)  # , eval_set=eval_set, early_stopping_rounds=10)  # , verbose = True)\n\nprediction_train = model_rf.predict(X_train_model)\nmse = mean_squared_error(prediction_train, y_train_model)\nr2 = model.score(X_train_model,y_train_model)\nprint('{}: MSE is {} and R2 is {}'.format('random forest on Train', round(mse,2), round(r2,2)))\nprint('\\n')\n\nprediction = model_rf.predict(X_test_model)\nmse = mean_squared_error(prediction, y_test_model)\nr2 = model_rf.score(X_test_model,y_test_model)\nprint('{}: MSE is {} and R2 is {}'.format('random forest on Test', round(mse,2), round(r2,2)))\nprint('\\n')\n\n\n# I do not see any overfitting in the model. The model is underfitting and has scope for learning.","metadata":{"execution":{"iopub.status.busy":"2023-04-05T05:28:38.180372Z","iopub.execute_input":"2023-04-05T05:28:38.180998Z","iopub.status.idle":"2023-04-05T05:28:41.804532Z","shell.execute_reply.started":"2023-04-05T05:28:38.180879Z","shell.execute_reply":"2023-04-05T05:28:41.803134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save the XGBoost model and Kmeans clustering model","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:46:28.710165Z","iopub.execute_input":"2023-04-05T04:46:28.710596Z","iopub.status.idle":"2023-04-05T04:46:28.716124Z","shell.execute_reply.started":"2023-04-05T04:46:28.710556Z","shell.execute_reply":"2023-04-05T04:46:28.714731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction","metadata":{"execution":{"iopub.status.busy":"2023-04-05T05:38:09.420584Z","iopub.execute_input":"2023-04-05T05:38:09.422125Z","iopub.status.idle":"2023-04-05T05:38:09.43678Z","shell.execute_reply.started":"2023-04-05T05:38:09.422068Z","shell.execute_reply":"2023-04-05T05:38:09.435187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Inferences using XGBoost model\n# Input will be time of day {0-23} and location cluster. But, we do not get this as input. We have input as datetime column and drop\n##  location coordinates. These will be passed as input and we will derive time of day and location cluster using K and then predict\n##  demand percentage.\n\ndef inferences(pickup_longitude, pickup_latitude, tpep_pickup_datetime):  # model, model_cluster\n    df_predict = pd.DataFrame(data= [[pickup_latitude, pickup_longitude]], columns=[\"pickup_latitude\", \"pickup_longitude\"])\n    df_predict['cluster_label'] = kmeans.predict(df_predict)\n    \n    df_predict.insert(loc = 0, column = 'tpep_pickup_datetime',value = [tpep_pickup_datetime])\n    df_predict[\"tpep_pickup_datetime\"] = df_predict[\"tpep_pickup_datetime\"].apply(pd.to_datetime)\n    df_predict['pickup_hour'] = df_predict[\"tpep_pickup_datetime\"].dt.hour\n    df_predict['pickup_dayofweek'] = df_predict[\"tpep_pickup_datetime\"].dt.dayofweek\n\n    cat_columns = ['pickup_dayofweek', 'pickup_hour', 'cluster_label']\n    datastruct_predict_encoded = enc.transform(df_predict[cat_columns]).toarray()\n    df_predict_encoded_df = pd.DataFrame(datastruct_predict_encoded, columns=enc.get_feature_names(cat_columns))\n    df_predict_model = pd.concat([df_predict.reset_index(), df_predict_encoded_df], axis=1).drop(['index'], axis=1)\n\n    df_predict_model = df_predict_model.drop(cat_columns, axis=1)\n    \n    # drop additional columner here.\n    cols_to_remove = ['pickup_dayofweek_0', 'pickup_dayofweek_1', 'pickup_dayofweek_2', 'pickup_dayofweek_3', 'pickup_dayofweek_4', 'pickup_dayofweek_5', 'pickup_dayofweek_6']\n    df_predict_model = df_predict_model.drop([\"pickup_latitude\", \"pickup_longitude\" , \"tpep_pickup_datetime\"], axis=1)\n    df_predict_model = df_predict_model.drop(cols_to_remove, axis=1)\n    \n\n    df_predict_model[df_predict_model.columns] = df_predict_model[df_predict_model.columns].astype(int)\n\n    # use XGboost with additional features removed.\n    prediction = model.predict(df_predict_model)\n    prediction = round(prediction[0]*100, 2)\n    return prediction\n    \nprint(\"The demand percentage based on location and time is {}%\".format(inferences(\"40.777981\", \"-73.980492\", \"2020-02-13 23:40:00\")))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T09:27:22.837043Z","iopub.execute_input":"2024-04-19T09:27:22.837508Z","iopub.status.idle":"2024-04-19T09:27:23.110909Z","shell.execute_reply.started":"2024-04-19T09:27:22.837471Z","shell.execute_reply":"2024-04-19T09:27:23.109174Z"},"trusted":true},"execution_count":null,"outputs":[]}]}